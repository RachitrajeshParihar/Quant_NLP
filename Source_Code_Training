# 1. INSTALLING THE DEPENDENCIES
pip install pandas numpy torch transformers tqdm

# 2. PREPROCESSING FOR MLP-MIXER
import pandas as pd
import numpy as np
import torch
from transformers import BertTokenizer, BertModel
from tqdm import tqdm

# Load dataset from the specified path and remove any missing values
dataset_path = "/kaggle/input/scraping-now/sentiment001.csv"
df = pd.read_csv(dataset_path).dropna()

# Load the pre-trained BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = BertModel.from_pretrained("bert-base-uncased")

# Set the device to GPU if available, otherwise use CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert_model.to(device)

def get_bert_embeddings(text_list, batch_size=32):
    """
    Converts a list of text inputs into BERT embeddings using batch processing.
    
    Parameters:
        text_list (list): List of text samples (tweets)
        batch_size (int): Number of samples per batch for efficient processing

    Returns:
        numpy.ndarray: BERT embeddings for the given text inputs
    """
    embeddings = []
    for i in tqdm(range(0, len(text_list), batch_size), desc="Embedding"):
        batch_texts = text_list[i : i + batch_size]
        
        # Tokenize and convert text into input tensors
        tokens = tokenizer(batch_texts, padding="max_length", truncation=True, 
                           max_length=512, return_tensors="pt")
        
        # Move tokens to the selected device (GPU/CPU)
        tokens = {k: v.to(device) for k, v in tokens.items()}
        
        with torch.no_grad():
            outputs = bert_model(**tokens)
        
        # Compute mean of the last hidden state to obtain fixed-size embeddings
        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
        embeddings.append(batch_embeddings)
    
    # Combine all batches into a single array
    return np.vstack(embeddings)

# Compute BERT embeddings for all tweets
X_embeddings = get_bert_embeddings(df["tweet"].tolist(), batch_size=64)

# Extract sentiment labels from the dataset
y = df["sentiment"].values  

def reshape_to_patches(data, patch_size=16):
    """
    Reshapes BERT embeddings into a format suitable for MLP-Mixer by dividing them into patches.
    
    Parameters:
        data (numpy.ndarray): Input feature matrix of shape (num_samples, embedding_dim)
        patch_size (int): Size of each patch

    Returns:
        numpy.ndarray: Reshaped feature matrix of shape (num_samples, num_patches, patch_size)
    """
    num_samples, embedding_dim = data.shape
    num_patches = embedding_dim // patch_size
    return data.reshape((num_samples, num_patches, patch_size))

# Transform BERT embeddings into patches for MLP-Mixer model
X_patches = reshape_to_patches(X_embeddings, patch_size=16)

# Save processed data as NumPy arrays
np.save("/kaggle/working/X.npy", X_patches)
np.save("/kaggle/working/y.npy", y)

print("Preprocessing Complete! Data saved as /kaggle/working/X.npy and /kaggle/working/y.npy")


# 3. MODEL TRAINING

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from tqdm import tqdm
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import accuracy_score, mean_squared_error
import scipy.stats

# Load preprocessed dataset
X_path = "/kaggle/working/X.npy"
y_path = "/kaggle/working/y.npy"

X = np.load(X_path)  # Feature matrix with shape (num_samples, num_patches, patch_dim)
y = np.load(y_path)  # Labels with shape (num_samples,)

# Convert NumPy arrays to PyTorch tensors
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.long)  # Modify dtype if performing regression

# Create dataset and data loader for training
batch_size = 32  
dataset = TensorDataset(X_tensor, y_tensor)
train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Define the MLP-Mixer model
class MLPMixer(nn.Module):
    def __init__(self, num_patches=32, patch_dim=16, hidden_dim=256, num_classes=3, depth=4):
        """
        Implements an MLP-Mixer model for classification or regression.

        Parameters:
            num_patches (int): Number of patches obtained from the input.
            patch_dim (int): Dimensionality of each patch.
            hidden_dim (int): Hidden layer size for MLP blocks.
            num_classes (int): Number of output classes (1 for regression).
            depth (int): Number of Mixer layers.
        """
        super(MLPMixer, self).__init__()

        # Token Mixing MLP layers
        self.token_mixing = nn.Sequential(
            *[nn.Sequential(
                nn.LayerNorm(num_patches),
                nn.Linear(num_patches, hidden_dim),
                nn.GELU(),
                nn.Linear(hidden_dim, num_patches)
            ) for _ in range(depth)]
        )

        # Channel Mixing MLP layers
        self.channel_mixing = nn.Sequential(
            *[nn.Sequential(
                nn.LayerNorm(patch_dim),
                nn.Linear(patch_dim, hidden_dim),
                nn.GELU(),
                nn.Linear(hidden_dim, patch_dim)
            ) for _ in range(depth)]
        )

        # Final classification layer
        self.fc = nn.Linear(num_patches * patch_dim, num_classes)

    def forward(self, x):
        """
        Forward pass for the MLP-Mixer.

        Parameters:
            x (Tensor): Input tensor with shape (batch_size, num_patches, patch_dim).

        Returns:
            Tensor: Model output logits.
        """
        x = x.permute(0, 2, 1)  # Transpose to (batch, patch_dim, num_patches)
        x = self.token_mixing(x) + x  # Apply token mixing with residual connection
        x = x.permute(0, 2, 1)  # Transpose back to (batch, num_patches, patch_dim)
        x = self.channel_mixing(x) + x  # Apply channel mixing with residual connection
        x = x.flatten(1)  # Flatten for classification
        return self.fc(x)

# Initialize model and move it to the appropriate device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_classes = len(set(y)) if len(set(y)) > 1 else 1  # Use regression if there is only one unique label
model = MLPMixer(num_patches=X.shape[1], patch_dim=X.shape[2], num_classes=num_classes)
model.to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss() if num_classes > 1 else nn.MSELoss()  # Classification or regression loss
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# Training loop
epochs = 20  
for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch+1}: Loss = {running_loss/len(train_loader):.4f}")

# Save the trained model
model_path = "/kaggle/working/mlp_mixer_model.pth"
torch.save(model.state_dict(), model_path)
print(f"Training Complete! Model saved at {model_path}")

# Evaluation
model.eval()
y_true, y_pred = [], []

with torch.no_grad():
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        preds = torch.argmax(outputs, dim=1).cpu().numpy() if num_classes > 1 else outputs.cpu().numpy()
        
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(preds)

# Compute evaluation metrics
accuracy = accuracy_score(y_true, y_pred) if num_classes > 1 else None
rmse = mean_squared_error(y_true, y_pred, squared=False)
correlation, _ = scipy.stats.pearsonr(y_true, y_pred)

# Display evaluation results
print("\nEvaluation Results:")
if accuracy is not None:
    print(f"Accuracy: {accuracy:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"Correlation: {correlation:.4f}")
